{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNTK - Hand Writing Recognition Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0rc2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make sure CNTK is available and up to date\n",
    "import cntk as C\n",
    "C.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batches are represented either as dense numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "# ...or as scipy compressed sparse matrices (CSR)\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# helper package for this tutorial (data loading, manual testing, etc.)\n",
    "import dlt\n",
    "\n",
    "# other usefel stuff\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cntk.device import set_default_device, gpu, cpu\n",
    "\n",
    "# All you really need is your laptop CPU\n",
    "set_default_device(cpu())\n",
    "\n",
    "# ...but deep learning on a GPU is way more fun!\n",
    "# set_default_device(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# name the experiment, to distinguish runs\n",
    "your_name = 'test'\n",
    "xp_name = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each sample is of size 256 (dimension 1)\n",
    "\n",
    "# You can think of it as a flattened square of black and white pixels of size 16x16\n",
    "\n",
    "# In the training datasets there are 93000 samples\n",
    "# We are going to break them into batches\n",
    "train = dlt.load_hdf5('/data/uji/train.hdf')\n",
    "# print(train.x.shape)  # (93000, 256)\n",
    "# print(train.y.shape)  # (93000,)\n",
    "\n",
    "# print(train.x[0, :])           # [ 0.00 0.00 ... ] - a single example (256-vector of floats [0 1])#\n",
    "# print(train.y[0])              # 21 - label representation\n",
    "# print(train.vocab[train.y[0]]) # L  - actual label\n",
    "\n",
    "# In the validation dataset there are only 620 samples\n",
    "# A single batch is fine if you've got the memory (no backprop on it anyway)\n",
    "valid = dlt.load_hdf5('/data/uji/valid.hdf')\n",
    "# print(valid.x.shape)  # (620, 256)\n",
    "# print(valid.y.shape)  # (620,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot little detour\n",
    "\n",
    "# if your data is one-hot indices, like target ids, it's better to represent it as a sparse matrix.\n",
    "# this is a snippet taken from the CNTK documentation that converts a list of indices to a compressed sparse matrix\n",
    "# there might be better ways, if you find one let me know!\n",
    "def seq_to_csr_matrix(seq, vocab_size):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    for term_idx in seq:\n",
    "        indices.append(term_idx)\n",
    "        data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return csr_matrix((data, indices, indptr), shape=(len(seq), vocab_size), dtype=np.float32)\n",
    "\n",
    "# much more efficient, however, is to incrementally create CSR arrays.\n",
    "# see https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "# for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_features 256\n",
      "label_classes 61\n"
     ]
    }
   ],
   "source": [
    "# data dependent hyper-parameters\n",
    "# (i.e. size of inputs and outputs)\n",
    "\n",
    "# what's the number of dimensions in input? (i.e. number of features)\n",
    "input_features = train.x.shape[1] # 256 (hint: they are all the same and it's a dimension in the batches)\n",
    "print('input_features', input_features)\n",
    "\n",
    "# what's the number of different letters we are trying to recognise? (i.e. number of labels)\n",
    "label_classes = max(l for l in train.y) # 61 (hint: it's the maximum label you can come across in the data)\n",
    "print('label_classes', label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train_batches 182\n",
      "full batch shape (512, 256)\n",
      "last batch shape (328, 256)\n"
     ]
    }
   ],
   "source": [
    "# how many examples do we show to the network before backpropating the error?\n",
    "# this number should be as big as you can make it without running out of memory, \n",
    "# BUT it might hurt your convergence in the long run if it's too big.\n",
    "batch_size = 512\n",
    "\n",
    "# how many training batches are there?\n",
    "n_train_batches = math.ceil(train.x.shape[0] / batch_size)\n",
    "print('n_train_batches', n_train_batches)\n",
    "\n",
    "# a list of tuples [(example, label), ...]\n",
    "train_batches = [(train.x[i*batch_size:(i+1)*batch_size, :],\n",
    "                  seq_to_csr_matrix(train.y[i*batch_size:(i+1)*batch_size].tolist(), label_classes))\n",
    "                 for i in range(n_train_batches)]\n",
    "\n",
    "# double check you've got them all\n",
    "assert n_train_batches == len(train_batches)\n",
    "\n",
    "# all batches have the same number of samples, exept the last one which might be a bit short\n",
    "for full_batch in train_batches[:-1]:\n",
    "    assert full_batch[0].shape[0] == batch_size\n",
    "    \n",
    "print('full batch shape', train_batches[0][0].shape)\n",
    "print('last batch shape', train_batches[-1][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wire up the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the simplest possible model is a single layer feed forward network (aka perceptron)\n",
    "\n",
    "# 1. a linear tranformation projects the input features into a hidden layer\n",
    "# 2. a non-linearity is applied\n",
    "# 3. a linear transformation projects from hidden_size to the number of output labels\n",
    "# (4.) our aim is to obtain a probability distribution over the labels, so a softmax would be applied here\n",
    "#      BUT in cntk the loss function (cross entropy) is coupled with the softmax, so no need to add it at this stage\n",
    "\n",
    "# cntk.layers has some useful stuff like \n",
    "#   - `Dense` for linear transformations\n",
    "#   - `Sequential` for concatenating layers\n",
    "# activations are passed in but in case you wonder they live in the main cntk package\n",
    "\n",
    "# for more info refer to \n",
    "def basic_feed_forward_model(hidden_size, activation_fn, label_classes):\n",
    "    return C.layers.Sequential ([\n",
    "                C.layers.Dense(hidden_size, activation=activation_fn),\n",
    "                C.layers.Dense(hidden_size, activation=activation_fn),\n",
    "                C.layers.Dense(label_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input variable for ingress data\n",
    "features = C.input_variable((input_features), np.float32)\n",
    "\n",
    "# input variable for label data (this is sparse)\n",
    "label = C.input_variable((label_classes), np.float32, is_sparse=True)\n",
    "\n",
    "# Instantiate the feedforward model\n",
    "model = basic_feed_forward_model(128, C.sigmoid, label_classes)\n",
    "\n",
    "# Apply the model to the input features\n",
    "z = model(features)\n",
    "\n",
    "# A training loss function + softmax (cross entropy and softmax)\n",
    "# parameters: applied network and the label input variable \n",
    "ce = C.cross_entropy_with_softmax(z, label)\n",
    "\n",
    "# An test time error function (i.e. classification)\n",
    "# parameters: applied network and the label input variable \n",
    "pe = C.classification_error(z, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the way the model is going to be trained.\n",
    "#   - what's the learning rate (schedule)?\n",
    "#   - what's the algorigthm for gradient descent?\n",
    "#   - what's the output you want to visualise during training?\n",
    "\n",
    "# https://www.cntk.ai/pythondocs/cntk.learners.html\n",
    "\n",
    "# learning rate is specified using a schedule, so that it can vary automatically during training.\n",
    "# you also need to decide if it should be relative to the `minibatches` or the `samples`\n",
    "lr_per_minibatch = C.learning_rate_schedule(0.2, C.UnitType.minibatch)\n",
    "\n",
    "# TensorBoardProgressWriterhelps visualising what's going on during trainining\n",
    "# make sure to set log_dir='/logs/<name_of_experiment>' and also to specify a model\n",
    "# https://www.cntk.ai/pythondocs/cntk.logging.html\n",
    "logger = C.logging.TensorBoardProgressWriter(freq=None, log_dir='/logs/%s'%xp_name, model=z)\n",
    "\n",
    "# setup the algorithm for gradient descent (in cntk speak: learner)\n",
    "learner = C.learners.adam(z.parameters, lr=lr_per_minibatch, momentum=C.momentum_schedule(0.9))\n",
    "\n",
    "# a trainer takes care of all your training needs (sort of)\n",
    "trainer = C.Trainer(z, (ce, pe), [learner], [logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 57277 parameters in 6 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "# how big is your model? (aka number of parameters)\n",
    "C.logging.log_number_of_parameters(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, validation accuracy: 73.06%\n",
      "Epoch 1, validation accuracy: 73.06%\n",
      "Epoch 2, validation accuracy: 73.06%\n",
      "Epoch 3, validation accuracy: 72.90%\n",
      "Epoch 4, validation accuracy: 72.90%\n",
      "Epoch 5, validation accuracy: 72.90%\n",
      "Epoch 6, validation accuracy: 73.06%\n",
      "Epoch 7, validation accuracy: 73.06%\n",
      "Epoch 8, validation accuracy: 72.90%\n",
      "Epoch 9, validation accuracy: 72.90%\n",
      "Epoch 10, validation accuracy: 73.06%\n",
      "Epoch 11, validation accuracy: 72.90%\n",
      "Epoch 12, validation accuracy: 72.90%\n",
      "Epoch 13, validation accuracy: 72.74%\n",
      "Epoch 14, validation accuracy: 72.90%\n",
      "Epoch 15, validation accuracy: 72.90%\n",
      "Epoch 16, validation accuracy: 72.74%\n",
      "Epoch 17, validation accuracy: 72.74%\n",
      "Epoch 18, validation accuracy: 72.74%\n",
      "Epoch 19, validation accuracy: 72.74%\n",
      "Epoch 20, validation accuracy: 72.74%\n",
      "Epoch 21, validation accuracy: 72.90%\n",
      "Epoch 22, validation accuracy: 73.06%\n",
      "Epoch 23, validation accuracy: 72.90%\n",
      "Epoch 24, validation accuracy: 72.90%\n",
      "Epoch 25, validation accuracy: 72.90%\n",
      "Epoch 26, validation accuracy: 72.90%\n",
      "Epoch 27, validation accuracy: 72.58%\n",
      "Epoch 28, validation accuracy: 72.58%\n",
      "Epoch 29, validation accuracy: 72.58%\n",
      "Epoch 30, validation accuracy: 72.74%\n",
      "Epoch 31, validation accuracy: 72.58%\n",
      "Epoch 32, validation accuracy: 72.58%\n",
      "Epoch 33, validation accuracy: 72.74%\n",
      "Epoch 34, validation accuracy: 72.90%\n",
      "Epoch 35, validation accuracy: 73.39%\n",
      "Epoch 36, validation accuracy: 73.39%\n",
      "Epoch 37, validation accuracy: 73.39%\n",
      "Epoch 38, validation accuracy: 73.23%\n",
      "Epoch 39, validation accuracy: 73.06%\n",
      "Epoch 40, validation accuracy: 72.90%\n",
      "Epoch 41, validation accuracy: 72.90%\n",
      "Epoch 42, validation accuracy: 72.90%\n",
      "Epoch 43, validation accuracy: 73.06%\n",
      "Epoch 44, validation accuracy: 73.06%\n",
      "Epoch 45, validation accuracy: 73.06%\n",
      "Epoch 46, validation accuracy: 73.06%\n",
      "Epoch 47, validation accuracy: 72.90%\n",
      "Epoch 48, validation accuracy: 72.90%\n",
      "Epoch 49, validation accuracy: 73.06%\n",
      "Epoch 50, validation accuracy: 73.23%\n",
      "Epoch 51, validation accuracy: 73.23%\n",
      "Epoch 52, validation accuracy: 73.23%\n",
      "Epoch 53, validation accuracy: 72.90%\n",
      "Epoch 54, validation accuracy: 72.90%\n",
      "Epoch 55, validation accuracy: 72.90%\n",
      "Epoch 56, validation accuracy: 72.90%\n",
      "Epoch 57, validation accuracy: 72.90%\n",
      "Epoch 58, validation accuracy: 72.74%\n",
      "Epoch 59, validation accuracy: 72.58%\n",
      "Epoch 60, validation accuracy: 72.42%\n",
      "Epoch 61, validation accuracy: 72.42%\n",
      "Epoch 62, validation accuracy: 72.58%\n",
      "Epoch 63, validation accuracy: 72.42%\n",
      "Epoch 64, validation accuracy: 72.42%\n",
      "Epoch 65, validation accuracy: 72.42%\n",
      "Epoch 66, validation accuracy: 72.42%\n",
      "Epoch 67, validation accuracy: 72.26%\n",
      "Epoch 68, validation accuracy: 72.26%\n",
      "Epoch 69, validation accuracy: 72.26%\n",
      "Epoch 70, validation accuracy: 72.10%\n",
      "Epoch 71, validation accuracy: 72.10%\n",
      "Epoch 72, validation accuracy: 72.10%\n",
      "Epoch 73, validation accuracy: 71.77%\n",
      "Epoch 74, validation accuracy: 71.94%\n",
      "Epoch 75, validation accuracy: 72.10%\n",
      "Epoch 76, validation accuracy: 72.10%\n",
      "Epoch 77, validation accuracy: 72.10%\n",
      "Epoch 78, validation accuracy: 72.10%\n",
      "Epoch 79, validation accuracy: 71.77%\n",
      "Epoch 80, validation accuracy: 71.61%\n",
      "Epoch 81, validation accuracy: 71.61%\n",
      "Epoch 82, validation accuracy: 71.61%\n",
      "Epoch 83, validation accuracy: 71.94%\n",
      "Epoch 84, validation accuracy: 71.94%\n",
      "Epoch 85, validation accuracy: 71.94%\n",
      "Epoch 86, validation accuracy: 71.94%\n",
      "Epoch 87, validation accuracy: 71.77%\n",
      "Epoch 88, validation accuracy: 71.29%\n",
      "Epoch 89, validation accuracy: 71.45%\n",
      "Epoch 90, validation accuracy: 71.45%\n",
      "Epoch 91, validation accuracy: 71.45%\n",
      "Epoch 92, validation accuracy: 71.29%\n",
      "Epoch 93, validation accuracy: 71.13%\n",
      "Epoch 94, validation accuracy: 71.29%\n",
      "Epoch 95, validation accuracy: 71.29%\n",
      "Epoch 96, validation accuracy: 71.29%\n",
      "Epoch 97, validation accuracy: 71.29%\n",
      "Epoch 98, validation accuracy: 71.13%\n",
      "Epoch 99, validation accuracy: 71.13%\n"
     ]
    }
   ],
   "source": [
    "# loop through the dataset many times\n",
    "for epoch in range(100):\n",
    "    \n",
    "    # show the model all the training batches and make it learn!\n",
    "    for train_batch, train_labels in train_batches:\n",
    "\n",
    "        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "        # the key names are the name given to the input layers\n",
    "        # the values are the actual batch and label\n",
    "        trainer.train_minibatch({features : [train_batch], label : [train_labels]})  \n",
    "    \n",
    "    trainer.summarize_training_progress()\n",
    "    \n",
    "    # now evaluate the model on the validation set\n",
    "    valid_batch, valid_labels = valid.x, seq_to_csr_matrix(valid.y, label_classes)\n",
    "    \n",
    "    valid_error = trainer.test_minibatch({features : [valid_batch], label : [valid_labels]})\n",
    "    \n",
    "    # After each epoch, print the accuracy on the validation set as a percentage\n",
    "    valid_accuracy = 100 * (1 - valid_error) \n",
    "    print('Epoch %d, validation accuracy: %.2f%%' % (epoch, valid_accuracy))\n",
    "    logger.write_value('valid_accuracy', valid_accuracy, epoch)\n",
    "    trainer.summarize_test_progress()\n",
    "    \n",
    "    # save model to file\n",
    "    os.makedirs(your_name, exist_ok=True)\n",
    "    trainer.save_checkpoint(os.path.join(your_name, '%s.cntk.model' % xp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO - classify img, shape 256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "        <canvas id=\"a909-input-canvas\" width=\"256\" height=\"256\"\n",
       "                style=\"border-style: solid;\">\n",
       "        </canvas>\n",
       "        <label id=\"a909-label\"\n",
       "               style=\"font-size: 12em; width: 1em; text-align: center;\"\n",
       "        >?</label>\n",
       "        <canvas id=\"a909-output-canvas\" width=\"16\" height=\"16\"\n",
       "            style=\"width: 256px; height: 256px;\">\n",
       "        </canvas>\n",
       "        <p>\n",
       "            <button id=\"a909-clear\"\n",
       "                style=\"width: 256px; height: 3em;\"\n",
       "            >Clear (middle click)</button>\n",
       "        </p>\n",
       "        <p><pre id=\"a909-error\" style=\"color: #f00;\"></pre></p>\n",
       "        <p><pre id=\"a909-output\"></pre></p>\n",
       "        <script type=\"text/javascript\">\n",
       "            (function () {\n",
       "                var custom_input_id = \"#a909\";\n",
       "                // requires custom_input_id predefined\n",
       "\n",
       "// Helpers\n",
       "\n",
       "function get_canvas_image(canvas) {\n",
       "    var data = canvas.getContext(\"2d\")\n",
       "        .getImageData(0, 0, canvas.width, canvas.height)\n",
       "        .data;\n",
       "    var result = new Array(canvas.width * canvas.height);\n",
       "    for (var i = 0; i < result.length; ++i) {\n",
       "        var r = data[4 * i + 0] / 255;\n",
       "        var g = data[4 * i + 1] / 255;\n",
       "        var b = data[4 * i + 2] / 255;\n",
       "        var a = data[4 * i + 3] / 255;\n",
       "        result[i] = a * (1 - r) * (1 - g) * (1 - b);\n",
       "    }\n",
       "    return result;\n",
       "}\n",
       "\n",
       "function set_canvas_image(canvas, values) {\n",
       "    var ctx = canvas.getContext(\"2d\");\n",
       "    var data = ctx.getImageData(0, 0, canvas.width, canvas.height);\n",
       "    for (var i = 0; i < values.length; ++i) {\n",
       "        var v = Math.round(255 * (1 - values[i]));\n",
       "        data.data[4 * i + 0] = v; // r\n",
       "        data.data[4 * i + 1] = v; // g\n",
       "        data.data[4 * i + 2] = v; // b\n",
       "        data.data[4 * i + 3] = 255; // a\n",
       "    }\n",
       "    ctx.putImageData(data, 0, 0);\n",
       "}\n",
       "\n",
       "function clear_canvas(canvas) {\n",
       "    var ctx = canvas.getContext(\"2d\");\n",
       "    ctx.fillStyle = \"#fff\";\n",
       "    ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
       "}\n",
       "\n",
       "// Get the point on a canvas (in pixel coordinates) from a mouse/touch event\n",
       "function mouse_points(e) {\n",
       "    var xscale = e.target.clientWidth / e.target.width;\n",
       "    var yscale = e.target.clientHeight / e.target.height;\n",
       "    if (e.offsetX !== undefined) {\n",
       "        // mouse event\n",
       "        return [[e.offsetX / xscale, e.offsetY / yscale]];\n",
       "    } else {\n",
       "        // touch event\n",
       "        var points = [];\n",
       "        $.each(e.originalEvent.changedTouches, function (i, o) {\n",
       "            var parent = o.target.getBoundingClientRect();\n",
       "            var x = o.clientX - parent.left;\n",
       "            var y = o.clientY - parent.top;\n",
       "            points.push([x / xscale, y / yscale])\n",
       "        });\n",
       "        return points;\n",
       "    }\n",
       "}\n",
       "\n",
       "// Classification\n",
       "\n",
       "function classify(strokes) {\n",
       "    function handleOutput(e) {\n",
       "        if (e.msg_type === \"error\") {\n",
       "            console.warn(e.content.ename + \": \" + e.content.evalue);\n",
       "            console.warn(e.content.traceback);\n",
       "            $(custom_input_id + \"-error\").text(\n",
       "                e.content.ename + \": \" +\n",
       "                e.content.evalue + \"\\n\" +\n",
       "                e.content.traceback.join(\"\\n\").replace(/\\[\\d(;\\d+)?m/g, \"\"));\n",
       "\n",
       "        } else if (e.msg_type === \"stream\") {\n",
       "            console.log(e.content.text);\n",
       "            var output = $(custom_input_id + \"-output\");\n",
       "            output.text(output.text() + e.content.text);\n",
       "\n",
       "        } else if (e.msg_type === \"execute_result\") {\n",
       "            // data comes back in JSON, but with surrounding single quotes,\n",
       "            // and double-escaped\n",
       "            var data = e.content.data[\"text/plain\"].replace(\"\\\\\\\\\", \"\\\\\");\n",
       "            var d = JSON.parse(data.substring(1, data.length - 1));\n",
       "            $(custom_input_id + \"-label\").text(d.y);\n",
       "            set_canvas_image($(custom_input_id +\"-output-canvas\")[0], d.x);\n",
       "            $(custom_input_id + \"-error\").text(\"\");\n",
       "\n",
       "        } else {\n",
       "            console.error(\"Unknown IPython kernel message\", e);\n",
       "        }\n",
       "    }\n",
       "    $(custom_input_id + \"-output\").text(\"\");\n",
       "    $(custom_input_id + \"-error\").text(\"\");\n",
       "    var payload = {\n",
       "        id: custom_input_id.substring(1),\n",
       "        strokes: strokes\n",
       "    };\n",
       "    IPython.notebook.kernel.execute(\n",
       "        \"import dlt;\\n\" +\n",
       "        \"dlt.CustomInput._id_classify('\" + JSON.stringify(payload) + \"')\",\n",
       "        {iopub: {output: handleOutput}}, {silent: false});\n",
       "}\n",
       "\n",
       "\n",
       "// Script\n",
       "\n",
       "var settings = {\n",
       "    line_thickness: 10,\n",
       "    line_reps: 1\n",
       "};\n",
       "\n",
       "var stroke_in_progress = false;\n",
       "var strokes = [];\n",
       "function start_stroke(e) {\n",
       "    strokes.push([]);\n",
       "    $.each(mouse_points(e), function(i, p) {\n",
       "        strokes[strokes.length - 1].push(p);\n",
       "    });\n",
       "    stroke_in_progress = true;\n",
       "}\n",
       "function finish_stroke() {\n",
       "    if (stroke_in_progress) {\n",
       "        classify(strokes);\n",
       "        stroke_in_progress = false;\n",
       "    }\n",
       "}\n",
       "function move_stroke(e) {\n",
       "    if (stroke_in_progress) {\n",
       "        e.preventDefault();\n",
       "        var points = mouse_points(e);\n",
       "        $.each(points, function(i, p) {\n",
       "            strokes[strokes.length - 1].push(p);\n",
       "        });\n",
       "\n",
       "        // full redraw\n",
       "        var ctx = e.target.getContext(\"2d\");\n",
       "        ctx.fillStyle = \"#fff\";\n",
       "        ctx.fillRect(0, 0, e.target.width, e.target.height);\n",
       "        ctx.strokeStyle = \"#000\";\n",
       "        ctx.lineWidth = settings.line_thickness;\n",
       "        ctx.lineCap = \"round\";\n",
       "        ctx.beginPath();\n",
       "        $.each(strokes, function (i, stroke) {\n",
       "            ctx.moveTo(stroke[0][0], stroke[0][1]);\n",
       "            $.each(stroke, function (j, p) {\n",
       "                ctx.lineTo(p[0], p[1]);\n",
       "            });\n",
       "        });\n",
       "        ctx.stroke();\n",
       "        return true;\n",
       "    }\n",
       "}\n",
       "function reset_stroke() {\n",
       "    strokes = [];\n",
       "    stroke_in_progress = false;\n",
       "    clear_canvas($(custom_input_id + \"-input-canvas\")[0]);\n",
       "    clear_canvas($(custom_input_id + \"-output-canvas\")[0]);\n",
       "    $(custom_input_id + \"-label\").text(\" \");\n",
       "    $(custom_input_id + \"-error\").text(\"\");\n",
       "    $(custom_input_id + \"-output\").text(\"\");\n",
       "}\n",
       "\n",
       "reset_stroke();\n",
       "$(custom_input_id + \"-clear\").click(reset_stroke);\n",
       "$(custom_input_id + \"-input-canvas\")\n",
       "    .mousedown(start_stroke)\n",
       "    .mousemove(move_stroke)\n",
       "    .mouseup(function (e) {\n",
       "        if (e.which == 1) {\n",
       "            finish_stroke(e);\n",
       "        } else if (e.which == 2) {\n",
       "            reset_stroke(e);\n",
       "        }\n",
       "    })\n",
       "    .mouseleave(finish_stroke)\n",
       "    .on(\"touchstart\", start_stroke)\n",
       "    .on(\"touchmove\", move_stroke)\n",
       "    .on(\"touchend\", finish_stroke)\n",
       "    .on(\"touchcancel\", finish_stroke);\n",
       "\n",
       "            })()\n",
       "        </script>\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "<dlt.custom_input.CustomInput at 0x7f0cdc019ef0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optional) demo - try your classifier \n",
    "\n",
    "def classify(img):\n",
    "    # Hint - if you need a batch dimension, try: img.reshape(1, -1)\n",
    "    print(\"TODO - classify img, shape %s\" % img.shape)\n",
    "    #scores = np.random.randn(1, len(train.vocab))  # TODO - replace with real scores (it's easier than it sounds!)\n",
    "    scores = z(img)\n",
    "    return train.vocab[np.argmax(scores)]\n",
    "\n",
    "# quick check\n",
    "assert classify(valid.x[0, :]) in train.vocab\n",
    "\n",
    "dlt.CustomInput(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
