{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNTK - Hand Writing Recognition Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's make sure CNTK is available and up to date\n",
    "import cntk as C\n",
    "C.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batches are represented either as dense numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "# ...or as scipy compressed sparse matrices (CSR)\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# helper package for this tutorial (data loading, manual testing, etc.)\n",
    "import dlt\n",
    "\n",
    "# other usefel stuff\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cntk.device import set_default_device, gpu, cpu\n",
    "\n",
    "# All you really need is your laptop CPU\n",
    "set_default_device(cpu())\n",
    "\n",
    "# ...but deep learning on a GPU is way more fun!\n",
    "# set_default_device(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# name the experiment, to distinguish runs\n",
    "xp_name = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each sample is of size 256 (dimension 1)\n",
    "\n",
    "# You can think of it as a flattened square of black and white pixels of size 16x16\n",
    "\n",
    "# In the training datasets there are 93000 samples\n",
    "# We are going to break them into batches\n",
    "train = dlt.load_hdf5('/data/uji/train.hdf')\n",
    "# print(train.x.shape)  # (93000, 256)\n",
    "# print(train.y.shape)  # (93000,)\n",
    "\n",
    "# print(train.x[0, :])           # [ 0.00 0.00 ... ] - a single example (256-vector of floats [0 1])#\n",
    "# print(train.y[0])              # 21 - label representation\n",
    "# print(train.vocab[train.y[0]]) # L  - actual label\n",
    "\n",
    "# In the validation dataset there are only 620 samples\n",
    "# A single batch is fine if you've got the memory (no backprop on it anyway)\n",
    "valid = dlt.load_hdf5('/data/uji/valid.hdf')\n",
    "# print(valid.x.shape)  # (620, 256)\n",
    "# print(valid.y.shape)  # (620,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot little detour\n",
    "\n",
    "# if your data is one-hot indices, like target ids, it's better to represent it as a sparse matrix.\n",
    "# this is a snippet taken from the CNTK documentation that converts a list of indices to a compressed sparse matrix\n",
    "# there might be better ways, if you find one let me know!\n",
    "def seq_to_csr_matrix(seq, vocab_size):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    for term_idx in seq:\n",
    "        indices.append(term_idx)\n",
    "        data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return csr_matrix((data, indices, indptr), shape=(len(seq), vocab_size), dtype=np.float32)\n",
    "\n",
    "# much more efficient, however, is to incrementally create CSR arrays.\n",
    "# see https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "# for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data dependent hyper-parameters\n",
    "# (i.e. size of inputs and outputs)\n",
    "\n",
    "# what's the number of dimensions in input? (i.e. number of features)\n",
    "input_features = ... # (hint: they are all the same and it's a dimension in the batches)\n",
    "print('input_features', input_features)\n",
    "\n",
    "# what's the number of different letters we are trying to recognise? (i.e. number of labels)\n",
    "label_classes = ... # (hint: it's the maximum label you can come across in the data)\n",
    "print('label_classes', label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many examples do we show to the network before backpropating the error?\n",
    "# this number should be as big as you can make it without running out of memory, \n",
    "# BUT it might hurt your convergence in the long run if it's too big.\n",
    "batch_size = ...\n",
    "\n",
    "# how many training batches are there?\n",
    "n_train_batches = ...\n",
    "print('n_train_batches', n_train_batches)\n",
    "\n",
    "# a list of tuples [(example, label), ...]\n",
    "train_batches = ...\n",
    "\n",
    "# double check you've got them all\n",
    "assert n_train_batches == len(train_batches)\n",
    "\n",
    "# all batches have the same number of samples, exept the last one which might be a bit short\n",
    "for full_batch in train_batches[:-1]:\n",
    "    assert full_batch[0].shape[0] == batch_size\n",
    "    \n",
    "print('full batch shape', train_batches[0][0].shape)\n",
    "print('last batch shape', train_batches[-1][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the simplest possible model is a single layer feed forward network (aka perceptron)\n",
    "\n",
    "# 1. a linear tranformation projects the input features into a hidden layer\n",
    "# 2. a non-linearity is applied\n",
    "# 3. a linear transformation projects from hidden_size to the number of output labels\n",
    "# (4.) our aim is to obtain a probability distribution over the labels, so a softmax would be applied here\n",
    "#      BUT in cntk the loss function (cross entropy) is coupled with the softmax, so no need to add it at this stage\n",
    "\n",
    "# cntk.layers has some useful stuff like \n",
    "#   - `Dense` for linear transformations\n",
    "#   - `Sequential` for concatenating layers\n",
    "# activations are passed in but in case you wonder they live in the main cntk package\n",
    "\n",
    "# for more info refer to \n",
    "def basic_feed_forward_model(hidden_size, activation_fn, label_classes):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input variable for ingress data\n",
    "features = ...\n",
    "\n",
    "# input variable for label data (this is sparse)\n",
    "label = ...\n",
    "\n",
    "# Instantiate the feedforward model\n",
    "model = ...\n",
    "\n",
    "# Apply the model to the input features\n",
    "z = ...\n",
    "\n",
    "# A training loss function + softmax (cross entropy and softmax)\n",
    "# parameters: applied network and the label input variable \n",
    "ce = ...\n",
    "\n",
    "# An test time error function (i.e. classification)\n",
    "# parameters: applied network and the label input variable \n",
    "pe = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the way the model is going to be trained.\n",
    "#   - what's the learning rate (schedule)?\n",
    "#   - what's the algorigthm for gradient descent?\n",
    "#   - what's the output you want to visualise during training?\n",
    "\n",
    "# https://www.cntk.ai/pythondocs/cntk.learners.html\n",
    "\n",
    "# learning rate is specified using a schedule, so that it can vary automatically during training.\n",
    "# you also need to decide if it should be relative to the `minibatches` or the `samples`\n",
    "lr_per_minibatch = ...\n",
    "\n",
    "# TensorBoardProgressWriter helps visualising what's going on during trainining\n",
    "# make sure to set log_dir='/logs/<name_of_experiment>' and also to specify a model\n",
    "# https://www.cntk.ai/pythondocs/cntk.logging.html\n",
    "logger = ...\n",
    "\n",
    "# setup the algorithm for gradient descent (in cntk speak: learner)\n",
    "learner = ...\n",
    "\n",
    "# a trainer takes care of all your training needs (sort of)\n",
    "trainer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how big is your model? (aka number of parameters)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop through the dataset many times\n",
    "for epoch in range(...):\n",
    "    \n",
    "    # show the model all the training batches and make it learn!\n",
    "    for train_batch, train_labels in train_batches:\n",
    "\n",
    "        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "        # the key names are the name given to the input layers\n",
    "        # the values are the actual batch and label\n",
    "        ...\n",
    "        \n",
    "    # summarize\n",
    "    ...\n",
    "    \n",
    "    # now evaluate the model on the validation set\n",
    "    valid_batch, valid_labels = ...\n",
    "    \n",
    "    valid_error = ...\n",
    "    \n",
    "    # After each epoch, print the accuracy on the validation set as a percentage\n",
    "    valid_accuracy = ... \n",
    "    print('Epoch %d, validation accuracy: %.2f%%' % (epoch, valid_accuracy))\n",
    "    logger.write_value('valid_accuracy', valid_accuracy, epoch)\n",
    "    \n",
    "    # summarize\n",
    "    ...\n",
    "    \n",
    "    # save model to file\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (Optional) demo - try your classifier \n",
    "\n",
    "def classify(img):\n",
    "    # Hint - if you need a batch dimension, try: img.reshape(1, -1)\n",
    "    print(\"TODO - classify img, shape %s\" % img.shape)\n",
    "    scores = ... # replace with real scores (it's easier than it sounds!)\n",
    "    return train.vocab[np.argmax(scores)]\n",
    "\n",
    "# quick check\n",
    "assert classify(valid.x[0, :]) in train.vocab\n",
    "\n",
    "dlt.CustomInput(classify)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
